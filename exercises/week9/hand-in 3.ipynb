{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-in 3 - Analysis of variational methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Existence, subdifferentials and duality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functionals are given (for $\\alpha > 0$ and $A \\in \\mathbb{R}^{2 \\times 2}$ an invertible matrix):\n",
    "\n",
    "* $J_1: \\mathbb{R} \\rightarrow \\mathbb{R}, u \\mapsto \\frac{1}2(u-f)^2 + \\alpha|u|$\n",
    "* $J_2: \\mathbb{R} \\rightarrow \\mathbb{R}, u \\mapsto |u-f| + \\alpha u^2$\n",
    "* $J_3: \\mathbb{R}^2 \\rightarrow \\mathbb{R}, u \\mapsto \\frac{1}2\\Vert A u - f\\Vert_{\\ell^2}^2  + \\alpha \\Vert u \\Vert_{\\ell^2}$\n",
    "\n",
    "For the optimisation problems $J_i(u) \\rightarrow \\min_u$ perform the following analysis:\n",
    "\n",
    "__Proof__, that a minimum exists (use the fundamental theorem of optimisation) and __proof__ that it is unique.\n",
    "\n",
    "__Compute__ the optimality conditions and thereof (using cases) a solution formula dependent on $f$. It holds for $p\\in\\partial\\left\\|u\\right\\|_{\\ell^2}$ that\n",
    "$$p = \\frac{u}{\\left\\|u\\right\\|_{\\ell^2}} \\text{ for } \\; u\\neq 0 \\text{, and }$$\n",
    "$$p \\in \\mathrm{B}_1(0) \\quad \\text{ for } \\; u = 0\\, \\qquad$$\n",
    "where $\\mathrm{B}_1(0)$ denotes the unit ball around $0$.\n",
    "\n",
    "Hint: Remark, that for $J_3$ no explicit solution formula can be given. Hence, use the following substitution\n",
    "$c:=\\frac{\\alpha}{\\left\\|u\\right\\|_{\\ell^2}}$ and provide a solution formula dependent on $c$ and $f$.\n",
    "        \n",
    "__Compute__ the corresponding Fenchel-dual optimisation problems. You may use that the dual problem corresponding to $\\min_u J(u)$ is given by $\\max_p -J^*(p)$ where $J^*$ is the convex-conjugate of $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Dual ROF denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Proof__ that the Rudin-Osher-Fatemi (ROF) minimization for denoising with $L^2$ data fidelity and TV regularisation:\n",
    "$$\\frac{1}{2}\\|u - f\\|_{L^2}^2 + \\alpha \\text{TV}(u)$$\n",
    "is equivalent (in the sense of the same local minima) to the following dual minimization problem\n",
    "$$J(g) := \\frac{1}{2} \\int_\\Omega \\left(\\alpha \\nabla \\cdot  g - f\\right)^2 \\rightarrow \\min_g$$\n",
    "under the constraint $\\lVert g \\rVert_{L^\\infty} \\leq 1$. This is a constrained quadratic optimisation problem. The constraint should be interpreted as $|g(x)|_{l^2}^2 \\leq 1,\\, \\forall x \\in \\Omega$.\n",
    "\n",
    "__Write code__ which performs the explicit discretisation\n",
    "$$g_{k+\\frac{1}{2}} = g_k + \\sigma \\: \\nabla \\left(\\alpha \\nabla \\cdot g_k -f\\right)$$\n",
    "$$g_{k+1} = \\Pi(g_{k+\\frac{1}{2}})\\qquad\\qquad\\quad$$\n",
    "where $\\Pi(g) := \\frac{g}{\\lVert g \\rVert}_{L^\\infty}$ denotes a projection onto the unit circle.\n",
    "\n",
    "__Test__ your implementation on a 1D step function with additional random Gaussian noise. \n",
    "\n",
    "__Compare__ the solutions for different values of $\\alpha$ and choose the step size $\\sigma$ adequately.\n",
    "\n",
    "Hint: Use the primal optimality condition of the ROF model (with exact, dual definition of TV), to be able to visualise the primal solution $u^*$ out of the corresponding, computed dual solution $g^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# grid \\Omega = [0,1]\n",
    "n = 100\n",
    "h = 1/(n-1)\n",
    "x = np.linspace(0,1,n) \n",
    "\n",
    "# define divergence / gradient\n",
    "grad = (np.diag(np.ones(n-1),1) - np.diag(np.ones(n),0))/h\n",
    "div = -grad.T\n",
    "\n",
    "# make data\n",
    "f = np.heaviside(x - 0.2,0) + 0.1*np.random.randn(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = np.zeros(n)\n",
    "sigma = 1\n",
    "alpha = 1\n",
    "for k in range(100):\n",
    "    g = g + sigma*grad@(alpha*div@g - f)\n",
    "    g = g/np.linalg.norm(g,np.inf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
